{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Advanced RAG Content Engine - Generic Version\n",
    "import os\n",
    "import logging\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Core imports\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# FIXED IMPORT - SemanticChunker is in experimental module\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "# Evaluation imports\n",
    "try:\n",
    "    from ragas import evaluate\n",
    "    from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "    from datasets import Dataset\n",
    "    RAGAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"RAGAS not available. Install with: pip install ragas\")\n",
    "    RAGAS_AVAILABLE = False\n",
    "\n",
    "# Advanced features\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class GenericContentEngine:\n",
    "    def __init__(self, pdf_path: str = 'pdfs/', persist_directory: str = 'db'):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.persist_directory = persist_directory\n",
    "        self.setup_logging()\n",
    "        self.metrics = {}\n",
    "        self.document_sources = set()  # Track document sources dynamically\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Enhanced logging setup\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('content_engine.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def load_and_process_documents(self) -> List[Document]:\n",
    "        \"\"\"Advanced document loading with metadata enrichment and source detection\"\"\"\n",
    "        self.logger.info(\"Loading documents...\")\n",
    "        loader = PyPDFDirectoryLoader(path=self.pdf_path, glob=\"**/*.pdf\")\n",
    "        pdfs = loader.load()\n",
    "        \n",
    "        # Enrich metadata and track sources\n",
    "        for doc in pdfs:\n",
    "            doc.metadata['load_timestamp'] = datetime.now().isoformat()\n",
    "            doc.metadata['word_count'] = len(doc.page_content.split())\n",
    "            \n",
    "            # Extract document source/company name from filename or content\n",
    "            source_file = doc.metadata.get('source', '').lower()\n",
    "            if source_file:\n",
    "                # Extract company/document type from filename\n",
    "                filename = os.path.basename(source_file)\n",
    "                self.document_sources.add(filename)\n",
    "            \n",
    "        self.logger.info(f\"Loaded {len(pdfs)} documents from sources: {list(self.document_sources)}\")\n",
    "        return pdfs\n",
    "    \n",
    "    def advanced_chunking(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Implement semantic chunking for better context preservation\"\"\"\n",
    "        # Traditional chunking\n",
    "        traditional_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, \n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "        traditional_chunks = traditional_splitter.split_documents(documents)\n",
    "        \n",
    "        # Semantic chunking - FIXED IMPLEMENTATION\n",
    "        try:\n",
    "            # Create embeddings for semantic chunker\n",
    "            embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "            \n",
    "            # Initialize semantic chunker with correct import\n",
    "            semantic_splitter = SemanticChunker(\n",
    "                embeddings=embeddings,\n",
    "                breakpoint_threshold_type=\"percentile\",\n",
    "                breakpoint_threshold_amount=95\n",
    "            )\n",
    "            \n",
    "            # Split documents using semantic chunker\n",
    "            semantic_chunks = []\n",
    "            for doc in documents:\n",
    "                chunks = semantic_splitter.split_text(doc.page_content)\n",
    "                for chunk in chunks:\n",
    "                    semantic_chunks.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={**doc.metadata, 'chunk_type': 'semantic'}\n",
    "                    ))\n",
    "            \n",
    "            # Mark traditional chunks\n",
    "            for chunk in traditional_chunks:\n",
    "                chunk.metadata['chunk_type'] = 'traditional'\n",
    "            \n",
    "            # Combine both approaches\n",
    "            all_chunks = traditional_chunks + semantic_chunks\n",
    "            self.logger.info(f\"Created {len(all_chunks)} chunks ({len(traditional_chunks)} traditional + {len(semantic_chunks)} semantic)\")\n",
    "            return all_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Semantic chunking failed: {e}, using traditional chunking only\")\n",
    "            return traditional_chunks\n",
    "    \n",
    "    def create_hybrid_retriever(self, documents: List[Document]) -> EnsembleRetriever:\n",
    "        \"\"\"Create hybrid retriever with vector + BM25\"\"\"\n",
    "        # Vector retriever\n",
    "        embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents, \n",
    "            embeddings, \n",
    "            persist_directory=self.persist_directory\n",
    "        )\n",
    "        vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "        \n",
    "        # BM25 retriever\n",
    "        bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        bm25_retriever.k = 10\n",
    "        \n",
    "        # Ensemble retriever (hybrid)\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[vector_retriever, bm25_retriever],\n",
    "            weights=[0.7, 0.3]  # Favor vector search slightly\n",
    "        )\n",
    "        \n",
    "        return ensemble_retriever\n",
    "    \n",
    "    def initialize_advanced_llm(self) -> ChatGroq:\n",
    "        \"\"\"Initialize LLM with advanced configuration\"\"\"\n",
    "        return ChatGroq(\n",
    "            groq_api_key=os.getenv('GROQ_API_KEY'),\n",
    "            model_name=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "    \n",
    "    def create_generic_prompt(self) -> PromptTemplate:\n",
    "        \"\"\"GENERIC prompt template that works with any documents\"\"\"\n",
    "        template = \"\"\"You are an expert document analyst with access to various PDF documents. Your task is to analyze the provided context and answer questions accurately based on the available information.\n",
    "\n",
    "Context Information:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the provided context carefully from the available documents\n",
    "2. Answer the question with specific details, numbers, and facts when available\n",
    "3. Cite the source document and page number when referencing specific information\n",
    "4. If comparing multiple entities/companies/topics, clearly distinguish between them\n",
    "5. If information is insufficient or missing, clearly state what's not available\n",
    "6. Provide reasoning and evidence for your conclusions\n",
    "7. Be objective and factual in your analysis\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Detailed Analysis and Answer:\"\"\"\n",
    "        \n",
    "        return PromptTemplate(\n",
    "            template=template, \n",
    "            input_variables=['question', 'context']\n",
    "        )\n",
    "    \n",
    "    def get_document_summary(self) -> str:\n",
    "        \"\"\"Generate a summary of loaded documents for context\"\"\"\n",
    "        if self.document_sources:\n",
    "            sources_list = \", \".join([os.path.splitext(source)[0] for source in self.document_sources])\n",
    "            return f\"Available documents: {sources_list}\"\n",
    "        return \"Multiple documents available for analysis\"\n",
    "    \n",
    "    def evaluate_rag_performance(self, questions: List[str], ground_truths: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate RAG system performance using RAGAS\"\"\"\n",
    "        if not RAGAS_AVAILABLE:\n",
    "            self.logger.warning(\"RAGAS not available. Skipping evaluation.\")\n",
    "            return {}\n",
    "            \n",
    "        try:\n",
    "            # Generate answers for evaluation\n",
    "            answers = []\n",
    "            contexts = []\n",
    "            \n",
    "            for question in questions:\n",
    "                docs = self.retriever.invoke(question)\n",
    "                context = \"\\n\".join([d.page_content for d in docs])\n",
    "                contexts.append([context])\n",
    "                \n",
    "                response = self.chain.invoke({\n",
    "                    \"question\": question,\n",
    "                    \"context\": context\n",
    "                })\n",
    "                answers.append(response.content)\n",
    "            \n",
    "            # Create evaluation dataset\n",
    "            eval_dataset = Dataset.from_dict({\n",
    "                \"question\": questions,\n",
    "                \"answer\": answers,\n",
    "                \"contexts\": contexts,\n",
    "                \"ground_truth\": ground_truths\n",
    "            })\n",
    "            \n",
    "            # Evaluate\n",
    "            result = evaluate(\n",
    "                eval_dataset,\n",
    "                metrics=[faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "            )\n",
    "            \n",
    "            self.metrics = result\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Evaluation failed: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def setup_complete_pipeline(self):\n",
    "        \"\"\"Setup the complete advanced pipeline\"\"\"\n",
    "        # Load and process documents\n",
    "        documents = self.load_and_process_documents()\n",
    "        chunks = self.advanced_chunking(documents)\n",
    "        \n",
    "        # Create hybrid retriever\n",
    "        self.retriever = self.create_hybrid_retriever(chunks)\n",
    "        \n",
    "        # Initialize LLM and prompt\n",
    "        self.llm = self.initialize_advanced_llm()\n",
    "        self.prompt = self.create_generic_prompt()  # GENERIC PROMPT\n",
    "        \n",
    "        # Create chain\n",
    "        self.chain = self.prompt | self.llm\n",
    "        \n",
    "        self.logger.info(\"Generic pipeline setup complete\")\n",
    "        self.logger.info(self.get_document_summary())\n",
    "\n",
    "# Initialize and run the generic system\n",
    "engine = GenericContentEngine()\n",
    "engine.setup_complete_pipeline()\n",
    "\n",
    "# Generic test questions that work with any documents\n",
    "test_questions = [\n",
    "    \"What are the main topics covered in the available documents?\",\n",
    "    \"What financial information is available in the documents?\",\n",
    "    \"What are the key risk factors mentioned across the documents?\",\n",
    "    \"Compare the business models or strategies mentioned in different documents\",\n",
    "    \"What revenue or financial metrics are reported in the documents?\"\n",
    "]\n",
    "\n",
    "print(\"=== GENERIC RAG SYSTEM TESTING ===\")\n",
    "print(f\"Document Sources: {list(engine.document_sources)}\")\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n--- Query {i}: {question} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Retrieve documents\n",
    "        docs = engine.retriever.invoke(question)\n",
    "        context = \"\\n\".join([d.page_content for d in docs])\n",
    "        \n",
    "        # Generate response\n",
    "        response = engine.chain.invoke({\n",
    "            \"question\": question,\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        print(f\"Answer: {response.content}\")\n",
    "        print(f\"Sources: {len(docs)} documents retrieved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {e}\")\n",
    "\n",
    "print(\"\\n=== GENERIC PIPELINE SETUP COMPLETE ===\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
